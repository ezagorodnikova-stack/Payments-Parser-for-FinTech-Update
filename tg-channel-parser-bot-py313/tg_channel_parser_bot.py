# tg_channel_parser_bot.py
# –î–í–ê –†–ï–ñ–ò–ú–ê:
# 1) ¬´–ü–∞—Ä—Å–∏–Ω–≥ —Ç–≥ –∫–∞–Ω–∞–ª–æ–≤¬ª ‚Äî —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞–º/–∫–ª—é—á–∞–º –∏ HTML (–ø–µ—Ä–≤—ã–µ 2 –∞–±–∑–∞—Ü–∞).
# 2) ¬´–ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤¬ª ‚Äî RSS/Atom/Sitemaps, —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–±–µ–∑ –ø—Ä–µ—Å–µ—Ç–æ–≤), –Ω–∞–¥—ë–∂–Ω—ã–π –ø–∞—Ä—Å –¥–∞—Ç + content:encoded, HTML-–≤—ã–≤–æ–¥.

import os
import re
import sys
import csv
import shlex
import asyncio
import logging
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import List, Tuple, Optional, Set, Dict
from html import escape as html_escape

from dotenv import load_dotenv
from jinja2 import Template
from bs4 import BeautifulSoup

from telethon import TelegramClient
from telethon.sessions import StringSession
from telethon.errors import (
    UsernameNotOccupiedError, UsernameInvalidError,
    ChannelPrivateError, ChatAdminRequiredError,
)
from telethon.tl.types import Message

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application, CommandHandler, MessageHandler, ConversationHandler,
    ContextTypes, CallbackQueryHandler, filters
)

# ---------- –õ–û–ì–ò ----------
logging.basicConfig(
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    level=logging.INFO,
)
log = logging.getLogger("parser-bot")

# ---------- –°–û–°–¢–û–Ø–ù–ò–Ø ----------
(MENU, LINK, PERIOD, KEYWORDS, SITE_SITES, SITE_PERIOD, SITE_CONFIRM) = range(7)

# ---------- –ö–û–ù–§–ò–ì ----------
load_dotenv()
BOT_TOKEN       = os.getenv("BOT_TOKEN")
API_ID          = int(os.getenv("API_ID", "0"))
API_HASH        = os.getenv("API_HASH")
SESSION_STRING  = os.getenv("TELETHON_SESSION")
DEFAULT_DAYS    = int(os.getenv("DEFAULT_DAYS", "30"))
RESULTS_LIMIT   = int(os.getenv("RESULTS_LIMIT", "5000"))
OUTPUT_DIR      = Path(os.getenv("OUTPUT_DIR", "output"))
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

WORK_ROOT       = Path("work")  # —Ä–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∂–∏–º–∞ ¬´—Å–∞–π—Ç—ã¬ª
WORK_ROOT.mkdir(parents=True, exist_ok=True)

if not (BOT_TOKEN and API_ID and API_HASH and SESSION_STRING):
    raise SystemExit("ERROR: BOT_TOKEN, API_ID, API_HASH, TELETHON_SESSION –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã –≤ .env")

# ---------- Telethon –∫–ª–∏–µ–Ω—Ç ----------
tg_client = TelegramClient(StringSession(SESSION_STRING), API_ID, API_HASH)

# ---------- HTML-—à–∞–±–ª–æ–Ω—ã ----------
HTML_TEMPLATE_TG = Template("""
<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8">
  <title>{{ title }}</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <style>
    body { font-family: -apple-system, system-ui, Segoe UI, Roboto, Arial, sans-serif; margin: 0; background: #0b0f19; color: #e8ebf5; }
    .wrap { max-width: 980px; margin: 0 auto; padding: 32px 16px; }
    .card { background: #12182b; border: 1px solid #1e2742; border-radius: 16px; padding: 16px 18px; margin: 12px 0; box-shadow: 0 10px 30px rgba(0,0,0,.25); }
    .muted { color: #a9b2d6; font-size: 13px; }
    .title { font-size: 24px; margin: 0 0 8px 0; }
    .pill { display: inline-block; background: #1c2440; border: 1px solid #2b355a; color: #b5c3ff; padding: 2px 10px; border-radius: 999px; margin-right: 6px; font-size: 12px;}
    .post-title { font-size: 16px; margin: 0; line-height: 1.45 }
    a { color: #8fb3ff; text-decoration: none; }
    a:hover { text-decoration: underline; }
    .header { margin-bottom: 16px; }
    .content p { margin: 0.6em 0; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="header">
      <h1 class="title">{{ channel_name }}</h1>
      <div class="muted">–ü–µ—Ä–∏–æ–¥: {{ period_str }} ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ: {{ total }}{% if chips %} ‚Ä¢ –°–ª–æ–≤–∞: {% for k in chips %}<span class="pill">{{ k }}</span>{% endfor %}{% endif %}</div>
    </div>
    {% for p in posts %}
    <div class="card">
      <div class="muted">{{ p['date'] }}</div>
      {% if p['link'] %}
        <h3 class="post-title"><a href="{{ p['link'] }}" target="_blank" rel="noopener noreferrer">–û—Ç–∫—Ä—ã—Ç—å –ø–æ—Å—Ç ‚Üí</a></h3>
      {% else %}
        <h3 class="post-title">–ü–æ—Å—Ç #{{ p['id'] }}</h3>
      {% endif %}
      <div class="content">{{ p['html'] | safe }}</div>
    </div>
    {% endfor %}
  </div>
</body>
</html>
""".strip())

HTML_TEMPLATE_SITES = Template("""
<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8">
  <title>{{ title }}</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <style>
    body { font-family: -apple-system, system-ui, Segoe UI, Roboto, Arial, sans-serif; margin: 0; background: #0b0f19; color: #e8ebf5; }
    .wrap { max-width: 980px; margin: 0 auto; padding: 32px 16px; }
    .card { background: #12182b; border: 1px solid #1e2742; border-radius: 16px; padding: 16px 18px; margin: 12px 0; box-shadow: 0 10px 30px rgba(0,0,0,.25); }
    .muted { color: #a9b2d6; font-size: 13px; }
    .title { font-size: 24px; margin: 0 0 8px 0; }
    .pill { display: inline-block; background: #1c2440; border: 1px solid #2b355a; color: #b5c3ff; padding: 2px 10px; border-radius: 999px; margin-right: 6px; font-size: 12px;}
    .post-title { font-size: 18px; margin: 0; line-height: 1.45 }
    a { color: #8fb3ff; text-decoration: none; }
    a:hover { text-decoration: underline; }
    .header { margin-bottom: 16px; }
    .content p { margin: 0.6em 0; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="header">
      <h1 class="title">–°–∞–π—Ç—ã ‚Äî –ø–æ–¥–±–æ—Ä–∫–∞</h1>
      <div class="muted">–ü–µ—Ä–∏–æ–¥: {{ period_str }} ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ: {{ total }}{% if chips %} ‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {% for k in chips %}<span class="pill">{{ k }}</span>{% endfor %}{% endif %}</div>
    </div>
    {% for p in posts %}
    <div class="card">
      <div class="muted">{{ p['date'] }} ‚Ä¢ {{ p['source'] }}</div>
      <h3 class="post-title">{% if p['link'] %}<a href="{{ p['link'] }}" target="_blank" rel="noopener noreferrer">{{ p['title'] or "–û—Ç–∫—Ä—ã—Ç—å —Å—Ç–∞—Ç—å—é ‚Üí" }}</a>{% else %}{{ p['title'] or "–°—Ç–∞—Ç—å—è" }}{% endif %}</h3>
      {% if p['html'] %}<div class="content">{{ p['html'] | safe }}</div>{% endif %}
    </div>
    {% endfor %}
  </div>
</body>
</html>
""".strip())

# ---------- –í–°–¢–†–û–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –°–ê–ô–¢–û–í (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω) ----------
EMBEDDED_SITE_PARSER_NAME = "embedded_site_parser.py"
EMBEDDED_SITE_PARSER_CODE = r'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os, re, ssl, csv, time
from datetime import datetime, timezone, timedelta
import urllib.parse as urlparse
import urllib.request as urlrequest
import xml.etree.ElementTree as ET
from email.utils import parsedate_to_datetime
from typing import Dict, List, Optional, Tuple

DEFAULT_UA = ("Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
              "(KHTML, like Gecko) Chrome/124.0 Safari/537.36")
SSL_CONTEXT = None

def root_url(url: str) -> str:
    p = urlparse.urlparse(url)
    scheme = p.scheme or "https"
    return f"{scheme}://{p.netloc}/" if p.netloc else url

def http_get(url: str, timeout: int = 25, headers: Optional[Dict[str, str]] = None) -> bytes:
    req = urlrequest.Request(url, headers=headers or {"User-Agent": DEFAULT_UA})
    with urlrequest.urlopen(req, timeout=timeout, context=SSL_CONTEXT) as resp:
        return resp.read()

def to_iso(dt: datetime) -> str:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def to_aware(dt: datetime) -> datetime:
    return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)

def parse_date_guess(s: str):
    s = (s or "").strip()
    if not s: return None
    # RFC-2822
    try:
        return to_aware(parsedate_to_datetime(s))
    except Exception:
        pass
    # ISO-like
    for pat in [r"^\d{4}-\d{2}-\d{2}(?:[ T]\d{2}:\d{2}(?::\d{2})?)?Z?$",
                r"^\d{4}/\d{2}/\d{2}$",
                r"^\d{1,2}\s+\w+\s+\d{4}"]:
        if re.match(pat, s):
            try:
                s2 = s.replace("/", "-").replace("T", " ").replace("Z", "")
                if len(s2) == 10: s2 += " 00:00"
                return to_aware(datetime.fromisoformat(s2))
            except Exception:
                pass
    return None

def parse_date_or_default(s: Optional[str], default_dt: datetime) -> datetime:
    if not s: return default_dt
    s = s.strip()
    try:
        return to_aware(parsedate_to_datetime(s))
    except Exception:
        pass
    try:
        s2 = s.replace("/", "-").replace("T", " ").replace("Z", "")
        if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s2):
            return datetime.fromisoformat(s2).replace(tzinfo=timezone.utc)
        return to_aware(datetime.fromisoformat(s2))
    except Exception:
        return default_dt

def ensure_dir(p): os.makedirs(p, exist_ok=True)

def parse_feed_xml(xml_bytes: bytes):
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return []
    items = []
    # RSS 2.0
    for it in root.findall(".//item"):
        title = (it.findtext("title") or "").strip()
        link = (it.findtext("link") or "").strip()
        # description –ò–õ–ò content:encoded
        desc = (it.findtext("description") or "").strip()
        cenc = it.findtext("{http://purl.org/rss/1.0/modules/content/}encoded")
        if cenc and (not desc or len(cenc) > len(desc)):
            desc = cenc
        pub_date = (it.findtext("pubDate") or it.findtext("date")
                    or it.findtext("{http://purl.org/dc/elements/1.1/}date"))
        items.append((title, link, pub_date, desc))
    # Atom
    ns_atom = "{http://www.w3.org/2005/Atom}"
    for it in root.findall(f".//{ns_atom}entry"):
        title = (it.findtext(f"{ns_atom}title") or "").strip()
        link_el = it.find(f"{ns_atom}link")
        link = link_el.get("href") if link_el is not None else ""
        pub_date = (it.findtext(f"{ns_atom}updated") or it.findtext(f"{ns_atom}published"))
        desc = it.findtext(f"{ns_atom}summary") or ""
        content_el = it.find(f"{ns_atom}content")
        if content_el is not None and (not desc or len(content_el.text or "") > len(desc)):
            desc = content_el.text or ""
        items.append((title, link, pub_date, desc))
    return items

def discover_feeds(html: str, base_root: str):
    feeds = set()
    # <link rel="alternate" ...>
    for m in re.finditer(r'<link[^>]+rel=["\'](?:alternate|feed)["\'][^>]+>', html, flags=re.I):
        tag = m.group(0)
        href_m = re.search(r'href=["\']([^"\']+)["\']', tag, flags=re.I)
        type_m = re.search(r'type=["\']([^"\']+)["\']', tag, flags=re.I)
        if href_m:
            href = href_m.group(1)
            if not href.startswith("http"): href = urlparse.urljoin(base_root, href)
            if not type_m or re.search(r'(rss|atom|xml)', type_m.group(1), flags=re.I):
                feeds.add(href)
    # —Ç–∏–ø–æ–≤—ã–µ –ø—É—Ç–∏
    for suffix in ["/feed", "/rss", "/rss.xml", "/atom.xml", "/feed.xml"]:
        feeds.add(urlparse.urljoin(base_root, suffix))
    return list(feeds)

def get_sitemap_links(base_root: str):
    try:
        rb = http_get(urlparse.urljoin(base_root, "robots.txt"))
        links = re.findall(r"(?im)^sitemap:\s*(\S+)$", rb.decode("utf-8", "ignore"))
        return links
    except Exception:
        return []

def parse_sitemap(xml_bytes: bytes):
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return []
    ns = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}
    urls = []
    for it in root.findall(".//sm:url", ns):
        loc = it.findtext("sm:loc", default="", namespaces=ns).strip()
        lastmod = it.findtext("sm:lastmod", default="", namespaces=ns).strip()
        urls.append((loc, lastmod))
    return urls

def collect_site(site_url: str, end_dt, start_dt, throttle=0.6, accept_undated=True, verbose=False, max_items=2000):
    collected, reason = [], ""
    base_root = root_url(site_url)
    try:
        try:
            html = http_get(base_root, timeout=25).decode("utf-8", "ignore")
        except Exception as e:
            return [], f"get_homepage_error:{e}"
        feeds = discover_feeds(html, base_root)
        if verbose: print(f"[feeds] {base_root} -> {len(feeds)} candidates")
        for f in feeds:
            try:
                fb = http_get(f, timeout=25)
                for (title, link, pub_date, desc) in parse_feed_xml(fb):
                    dt = parse_date_guess(pub_date)
                    if not dt and accept_undated:
                        dt = end_dt
                    if not dt or not (start_dt <= dt <= end_dt):
                        continue
                    if not link or not link.startswith("http"):
                        link = urlparse.urljoin(base_root, link or "/")
                    collected.append({
                        "title": (title or "").strip(),
                        "link": (link or "").strip(),
                        "date": to_iso(dt),
                        "summary": (desc or "").strip(),
                        "source": base_root.rstrip("/")
                    })
            except Exception as e:
                if verbose: print(f"[feed_error] {f}: {e}")
        if not collected:
            for sm in get_sitemap_links(base_root):
                try:
                    urls = parse_sitemap(http_get(sm, timeout=25))
                    for u, lastmod in urls:
                        dt = parse_date_guess(lastmod)
                        if not dt and accept_undated:
                            dt = end_dt
                        if not dt or not (start_dt <= dt <= end_dt):
                            continue
                        collected.append({
                            "title": "",
                            "link": (u or "").strip(),
                            "date": to_iso(dt),
                            "summary": "",
                            "source": base_root.rstrip("/")
                        })
                except Exception as e:
                    if verbose: print(f"[sitemap_error] {sm}: {e}")
        seen, deduped = set(), []
        for it in collected:
            lk = it["link"]
            if lk in seen: continue
            seen.add(lk); deduped.append(it)
        if len(deduped) > max_items: deduped = deduped[:max_items]
        return deduped, reason
    finally:
        time.sleep(throttle)

def write_csv(rows: List[Dict[str, str]], path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=["date","title","link","summary","source"])
        w.writeheader()
        for r in rows: w.writerow(r)

def run(sites: List[str], days: Optional[int], start: Optional[str], end: Optional[str],
        throttle: float = 0.6, accept_undated=True, max_items=2000, verbose=False,
        cafile: Optional[str] = None, insecure=False, out_dir: str = "output"):
    global SSL_CONTEXT
    if insecure:
        SSL_CONTEXT = ssl._create_unverified_context()
    else:
        SSL_CONTEXT = ssl.create_default_context(cafile=cafile) if cafile else ssl.create_default_context()

    end_dt = parse_date_or_default(end, datetime.now(timezone.utc))
    if days is not None and (not start and not end):
        start_dt = end_dt - timedelta(days=days)
    else:
        start_dt = parse_date_or_default(start, end_dt - timedelta(days=30))

    sites_norm = []
    for s in (sites or []):
        s = s.strip()
        if not s: continue
        if not s.startswith("http"): s = "https://" + s.lstrip("/")
        sites_norm.append(s)

    all_rows = []
    for s in sites_norm:
        rows, _ = collect_site(s, end_dt=end_dt, start_dt=start_dt,
                               throttle=throttle, accept_undated=accept_undated,
                               verbose=verbose, max_items=max_items)
        safe = re.sub(r"[^A-Za-z0-9_.-]+", "_", urlparse.urlparse(root_url(s)).netloc or "site")
        write_csv(rows, os.path.join(out_dir, f"{safe}.csv"))
        all_rows.extend(rows)
    write_csv(all_rows, os.path.join(out_dir, "all_sites.csv"))
'''

# ---------- –û–±—â–∏–µ —É—Ç–∏–ª–∏—Ç—ã ----------
def parse_channel_identifier(raw: str) -> str:
    raw = raw.strip()
    m = re.search(r"(?:t\.me/|@)([A-Za-z0-9_]{3,})/?$", raw)
    if m: return m.group(1)
    if re.fullmatch(r"[A-Za-z0-9_]{3,}", raw): return raw
    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Å—Å—ã–ª–∫—É/—é–∑–µ—Ä–Ω–µ–π–º. –ü—Ä–∏–º–µ—Ä: https://t.me/fintechfutures –∏–ª–∏ @fintechfutures")

def parse_period(text: str) -> Tuple[datetime, datetime]:
    now = datetime.now(timezone.utc)
    s = (text or "").strip().lower()
    if re.fullmatch(r"\d{1,4}", s):
        days = int(s); return now - timedelta(days=days), now
    dates = re.findall(r"\b(\d{4}-\d{2}-\d{2})\b", s)
    if len(dates) >= 2:
        d1 = datetime.fromisoformat(dates[0]).replace(tzinfo=timezone.utc)
        d2 = datetime.fromisoformat(dates[1]).replace(tzinfo=timezone.utc)
        start, end = sorted([d1, d2])
        return start, end + timedelta(days=1)
    elif len(dates) == 1:
        d1 = datetime.fromisoformat(dates[0]).replace(tzinfo=timezone.utc)
        return d1, now
    else:
        return now - timedelta(days=DEFAULT_DAYS), now

def normalize_keywords(text: str) -> List[str]:
    parts = [p.strip().lower() for p in re.split(r"[,;\n]", text or "") if p.strip()]
    seen, out = set(), []
    for p in parts:
        if p not in seen: seen.add(p); out.append(p)
    return out

def message_text(msg: Message) -> str:
    return msg.message or ""

def match_keywords(text: str, keywords: List[str]) -> bool:
    if not keywords: return True
    t = text.lower()
    return any(k in t for k in keywords)

def channel_permalink(username: Optional[str], msg_id: int) -> Optional[str]:
    return f"https://t.me/{username}/{msg_id}" if username else None

def first_paragraphs_html(raw_text: str, n: int = 2) -> Optional[str]:
    plain = BeautifulSoup(raw_text or "", "html.parser").get_text()
    t = plain.replace("\r\n", "\n").replace("\r", "\n").strip()
    if not t: return None
    paras = [p.strip() for p in re.split(r"\n\s*\n+", t) if p.strip()]
    if len(paras) < n:
        lines = [ln.strip() for ln in t.split("\n") if ln.strip()]
        paras = lines[:n]
    else:
        paras = paras[:n]
    html_parts = []
    for p in paras:
        p_html = html_escape(p).replace("\n", "<br>")
        html_parts.append(f"<p>{p_html}</p>")
    return "".join(html_parts) if html_parts else None

def render_html_tg(channel_name: str, period_str: str, chips: List[str], posts: List[dict]) -> str:
    return HTML_TEMPLATE_TG.render(
        title=f"{channel_name} ‚Äî –ø–æ–¥–±–æ—Ä–∫–∞",
        channel_name=channel_name,
        period_str=period_str,
        chips=chips,
        posts=posts,
        total=len(posts),
    )

def render_html_sites(period_str: str, sources_chips: List[str], posts: List[dict]) -> str:
    return HTML_TEMPLATE_SITES.render(
        title="–°–∞–π—Ç—ã ‚Äî –ø–æ–¥–±–æ—Ä–∫–∞",
        period_str=period_str,
        chips=sources_chips,
        posts=posts,
        total=len(posts),
    )

def safe_filename(s: str) -> str:
    s = re.sub(r"[^\w\-\.\s]", "_", s, flags=re.UNICODE).strip()
    return re.sub(r"\s+", "_", s)

# ---------- –£—Ç–∏–ª–∏—Ç—ã ¬´–°–∞–π—Ç—ã¬ª ----------
def user_workdir(user_id: int) -> Path:
    d = WORK_ROOT / f"user_{user_id}"
    (d / "output").mkdir(parents=True, exist_ok=True)
    return d

def ensure_embedded_script_on_disk(workdir: Path) -> Path:
    script_path = workdir / EMBEDDED_SITE_PARSER_NAME
    script_path.write_text(EMBEDDED_SITE_PARSER_CODE, encoding="utf-8")
    return script_path

async def run_site_script(args_list: List[str], workdir: Path, timeout_sec: int = 1200) -> Tuple[int, str, str]:
    script_path = ensure_embedded_script_on_disk(workdir)
    cmd = [sys.executable, str(script_path)] + args_list
    log.info("Running embedded site parser: %s", " ".join(shlex.quote(c) for c in cmd))
    proc = await asyncio.create_subprocess_exec(
        *cmd, cwd=str(workdir),
        stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE,
    )
    try:
        stdout_b, stderr_b = await asyncio.wait_for(proc.communicate(), timeout=timeout_sec)
    except asyncio.TimeoutError:
        proc.kill()
        return 124, "", "Timeout while running embedded site parser"
    return proc.returncode, stdout_b.decode("utf-8", errors="ignore"), stderr_b.decode("utf-8", errors="ignore")

def site_confirm_keyboard() -> InlineKeyboardMarkup:
    kb = [
        [InlineKeyboardButton("‚ñ∂Ô∏è –ó–∞–ø—É—Å—Ç–∏—Ç—å", callback_data="site:run")],
        [InlineKeyboardButton("‚ùå –û—Ç–º–µ–Ω–∞", callback_data="site:cancel")],
    ]
    return InlineKeyboardMarkup(kb)

def norm_urls_from_text(text: str) -> List[str]:
    urls = re.findall(r'https?://[^\s,]+', text or "", flags=re.I)
    bare = [u for u in re.split(r"[,\s]+", text or "") if u and not u.startswith("http")]
    urls += [("https://" + b.lstrip("/")) for b in bare if re.match(r"^[A-Za-z0-9\.\-]+\.[A-Za-z]{2,}$", b)]
    out, seen = [], set()
    for u in urls:
        uu = u.strip()
        if uu and uu not in seen:
            seen.add(uu); out.append(uu)
    return out

def build_site_args_from_context(ctx_ud: Dict) -> List[str]:
    args: List[str] = []
    urls: List[str] = ctx_ud.get("site_urls") or []
    if urls:
        args += ["--sites", ",".join(urls)]

    text_period: str = ctx_ud.get("site_period_text", "") or ""
    if re.fullmatch(r"\d{1,4}", text_period.strip()):
        args += ["--days", text_period.strip()]
    else:
        dates = re.findall(r"\b(\d{4}-\d{2}-\d{2})\b", text_period)
        if len(dates) >= 2:
            args += ["--start", dates[0], "--end", dates[1]]
        elif len(dates) == 1:
            args += ["--start", dates[0]]
        else:
            args += ["--days", str(DEFAULT_DAYS)]

    # –±–µ—Ä—ë–º —ç–ª–µ–º–µ–Ω—Ç—ã –±–µ–∑ –¥–∞—Ç—ã —Ç–æ–∂–µ
    args += ["--accept-undated"]

    # –≤—ã–≤–æ–¥
    args += ["--out", "output"]

    # TLS trust store
    try:
        import certifi
        args += ["--cafile", certifi.where()]
    except Exception:
        pass
    return args

def read_all_sites_csv(out_dir: Path) -> List[Dict[str, str]]:
    target = out_dir / "all_sites.csv"
    rows: List[Dict[str, str]] = []
    if target.exists():
        with open(target, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for r in reader:
                rows.append({k: (r.get(k) or "") for k in ["date","title","link","summary","source"]})
    else:
        for p in out_dir.glob("*.csv"):
            if p.name == "all_sites.csv": continue
            with open(p, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for r in reader:
                    rows.append({k: (r.get(k) or "") for k in ["date","title","link","summary","source"]})
    return rows

def site_rows_to_posts(rows: List[Dict[str, str]]) -> List[dict]:
    def parse_dt(s: str) -> datetime:
        s = (s or "").strip()
        try:
            if s.endswith("Z"):
                return datetime.fromisoformat(s.replace("Z","+00:00"))
            return datetime.fromisoformat(s)
        except Exception:
            return datetime.min.replace(tzinfo=timezone.utc)

    rows_sorted = sorted(rows, key=lambda r: parse_dt(r.get("date","")), reverse=True)
    posts = []
    for i, r in enumerate(rows_sorted, start=1):
        summary_html = first_paragraphs_html(r.get("summary",""), n=2)
        dt_disp = r.get("date","")
        try:
            if dt_disp:
                dt = parse_dt(dt_disp)
                if dt != datetime.min.replace(tzinfo=timezone.utc):
                    dt_disp = dt.astimezone(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
        except Exception:
            pass
        posts.append({
            "id": i,
            "date": dt_disp or "",
            "link": r.get("link",""),
            "title": r.get("title","").strip(),
            "source": re.sub(r"^https?://", "", (r.get("source","") or "")).strip("/"),
            "html": summary_html or ""
        })
    return posts

# ---------- –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é ----------
def main_menu_markup() -> InlineKeyboardMarkup:
    kb = [
        [InlineKeyboardButton("üì∞ –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤", callback_data="menu:site")],
        [InlineKeyboardButton("üì£ –ü–∞—Ä—Å–∏–Ω–≥ —Ç–≥ –∫–∞–Ω–∞–ª–æ–≤", callback_data="menu:tg")],
    ]
    return InlineKeyboardMarkup(kb)

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –í—ã–±–µ—Ä–∏ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:", reply_markup=main_menu_markup())
    return MENU

async def menu_choice(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    choice = query.data
    if choice == "menu:tg":
        await query.edit_message_text(
            "–†–µ–∂–∏–º: –ü–∞—Ä—Å–∏–Ω–≥ —Ç–≥ –∫–∞–Ω–∞–ª–æ–≤.\n"
            "–°–∫–∏–Ω—å —Å—Å—ã–ª–∫—É –Ω–∞ –∫–∞–Ω–∞–ª (–ø—Ä–∏–º–µ—Ä: https://t.me/fintechfutures –∏–ª–∏ @fintechfutures)."
        )
        return LINK
    elif choice == "menu:site":
        await query.edit_message_text(
            "–†–µ–∂–∏–º: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤.\n"
            "–ü—Ä–∏—à–ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∞–π—Ç—ã (—á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª/–∑–∞–ø—è—Ç—É—é), –Ω–∞–ø—Ä–∏–º–µ—Ä:\n"
            "https://finextra.com https://techcrunch.com\n\n"
            "–ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –ø—Ä–∏—Å–ª–∞—Ç—å .txt-—Ñ–∞–π–ª (–æ–¥–Ω–∞ —Å—Å—ã–ª–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ)."
        )
        return SITE_SITES
    else:
        await query.edit_message_text("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –≤—ã–±–æ—Ä. –ò—Å–ø–æ–ª—å–∑—É–π /start.")
        return ConversationHandler.END

# ---------- –¢–ì-–≤–µ—Ç–∫–∞ ----------
async def cancel(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–û–∫–µ–π, –æ—Ç–º–µ–Ω–∏–ª. /start ‚Äî –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –º–µ–Ω—é.")
    return ConversationHandler.END

async def parse_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–í—ã–±–µ—Ä–∏ —Ä–µ–∂–∏–º:", reply_markup=main_menu_markup())
    return MENU

def period_human(start_dt: datetime, end_dt: datetime) -> str:
    return f"{start_dt.date()} ‚Äî {(end_dt - timedelta(days=1)).date()}"

async def ask_period(update: Update, context: ContextTypes.DEFAULT_TYPE):
    raw = (update.message.text or "").strip()
    try:
        username = parse_channel_identifier(raw)
        context.user_data["channel_username"] = username
        await update.message.reply_text(
            "–ü–µ—Ä–∏–æ–¥: –Ω–∞–ø–∏—à–∏ –ª–∏–±–æ —á–∏—Å–ª–æ –¥–Ω–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, 30), –ª–∏–±–æ –¥–∞—Ç—ã:\n"
            "‚Ä¢ '2025-08-01 2025-08-27' –∏–ª–∏ '—Å 2025-08-01 –ø–æ 2025-08-27'\n"
            f"–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–µ {DEFAULT_DAYS} –¥–Ω–µ–π."
        )
        return PERIOD
    except ValueError as e:
        await update.message.reply_text(str(e))
        return LINK

async def ask_keywords(update: Update, context: ContextTypes.DEFAULT_TYPE):
    period_text = (update.message.text or "").strip()
    start_dt, end_dt = parse_period(period_text)
    context.user_data["period"] = (start_dt, end_dt)
    await update.message.reply_text(
        f"–û–∫! –ü–µ—Ä–∏–æ–¥: {period_human(start_dt, end_dt)}\n–¢–µ–ø–µ—Ä—å –ø—Ä–∏—à–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é). "
        "–ï—Å–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–º ‚Äî —Å–æ–±–µ—Ä—É –≤—Å–µ –ø–æ—Å—Ç—ã –∑–∞ –ø–µ—Ä–∏–æ–¥."
    )
    return KEYWORDS

async def run_parse_tg(update: Update, context: ContextTypes.DEFAULT_TYPE):
    kw_raw = update.message.text or ""
    keywords = normalize_keywords(kw_raw)
    username = context.user_data["channel_username"]
    start_dt, end_dt = context.user_data["period"]

    await update.message.reply_text("–ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä‚Ä¶ —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –∫–∞–Ω–∞–ª–∞—Ö.")
    try:
        entity = await tg_client.get_entity(username)
        chan_username = getattr(entity, "username", None)
        chan_title = getattr(entity, "title", username)

        matched: List[dict] = []
        count = 0

        async for msg in tg_client.iter_messages(entity, offset_date=end_dt, reverse=False):
            count += 1
            if count > RESULTS_LIMIT: break
            if not isinstance(msg, Message): continue
            msg_dt = msg.date
            if msg_dt.tzinfo is None: msg_dt = msg_dt.replace(tzinfo=timezone.utc)
            if msg_dt >= end_dt: continue
            if msg_dt < start_dt: break

            text = message_text(msg)
            if not text: continue

            if match_keywords(text, keywords):
                snippet_html = first_paragraphs_html(text, n=2)
                if not snippet_html: continue
                matched.append({
                    "id": msg.id,
                    "date": msg_dt.astimezone(timezone.utc).strftime("%Y-%m-%d %H:%M UTC"),
                    "link": channel_permalink(chan_username, msg.id),
                    "html": snippet_html
                })

        html = render_html_tg(chan_title, period_human(start_dt, end_dt), chips=keywords, posts=matched)

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        fname = f"{safe_filename(chan_title)}__{start_dt.date()}_{(end_dt - timedelta(days=1)).date()}__{ts}.html"
        fpath = OUTPUT_DIR / fname
        fpath.write_text(html, encoding="utf-8")

        await update.message.reply_document(
            document=fpath.open("rb"),
            filename=fname,
            caption=f"–ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(matched)}\n/start ‚Äî –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –º–µ–Ω—é."
        )

    except UsernameNotOccupiedError:
        await update.message.reply_text("–ö–∞–Ω–∞–ª —Å —Ç–∞–∫–∏–º username –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    except UsernameInvalidError:
        await update.message.reply_text("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π username –∫–∞–Ω–∞–ª–∞.")
    except ChannelPrivateError:
        await update.message.reply_text("–ö–∞–Ω–∞–ª –ø—Ä–∏–≤–∞—Ç–Ω—ã–π. –í–∞—à –∞–∫–∫–∞—É–Ω—Ç (Telethon) –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É—á–∞—Å—Ç–Ω–∏–∫–æ–º –∫–∞–Ω–∞–ª–∞.")
    except ChatAdminRequiredError:
        await update.message.reply_text("–ù—É–∂–Ω—ã –ø—Ä–∞–≤–∞ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∏—Å—Ç–æ—Ä–∏–∏ —ç—Ç–æ–≥–æ –∫–∞–Ω–∞–ª–∞.")
    except Exception as e:
        log.exception("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞")
        await update.message.reply_text(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
    return ConversationHandler.END

# ---------- –í–µ—Ç–∫–∞ ¬´–°–∞–π—Ç—ã¬ª ----------
async def site_collect_sites(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    workdir = user_workdir(user_id)

    urls: List[str] = []
    if update.message.document:
        doc = update.message.document
        if not (doc.mime_type or "").startswith("text/") and not doc.file_name.lower().endswith(".txt"):
            await update.message.reply_text("–≠—Ç–æ –Ω–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª. –ü—Ä–∏—à–ª–∏ .txt –∏–ª–∏ –Ω–∞–ø–∏—à–∏ —Å—Å—ã–ª–∫–∏ —Ç–µ–∫—Å—Ç–æ–º.")
            return SITE_SITES
        tmp = workdir / f"sites_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        file = await doc.get_file()
        await file.download_to_drive(custom_path=str(tmp))
        try:
            with open(tmp, "r", encoding="utf-8") as f:
                for line in f:
                    s = line.strip()
                    if s and not s.startswith("#"):
                        urls.extend(norm_urls_from_text(s))
        except Exception as e:
            await update.message.reply_text(f"–ù–µ —Å–º–æ–≥ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")
            return SITE_SITES
    else:
        urls = norm_urls_from_text(update.message.text or "")

    urls = list(dict.fromkeys(urls))
    if not urls:
        await update.message.reply_text("–ù–µ –Ω–∞—à—ë–ª –Ω–∏ –æ–¥–Ω–æ–π —Å—Å—ã–ª–∫–∏. –ü—Ä–∏—à–ª–∏ –µ—â—ë —Ä–∞–∑ —Å—Å—ã–ª–∫–∏ –∏–ª–∏ .txt-—Ñ–∞–π–ª.")
        return SITE_SITES

    context.user_data["site_urls"] = urls
    preview = "\n".join(f"‚Ä¢ {u}" for u in urls[:10])
    more = "" if len(urls) <= 10 else f"\n‚Ä¶–∏ –µ—â—ë {len(urls)-10}"
    await update.message.reply_text(
        f"–ü—Ä–∏–Ω—è–ª {len(urls)} —Å–∞–π—Ç(–æ–≤):\n{preview}{more}\n\n"
        "–¢–µ–ø–µ—Ä—å —É–∫–∞–∂–∏ –ø–µ—Ä–∏–æ–¥ (–∫–∞–∫ –¥–ª—è –¢–ì):\n"
        f"‚Ä¢ —á–∏—Å–ª–æ –¥–Ω–µ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä `30`\n"
        f"‚Ä¢ –∏–ª–∏ –¥–∞—Ç—ã: `2025-08-01 2025-08-27`",
        parse_mode="Markdown"
    )
    return SITE_PERIOD

async def site_collect_period(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    context.user_data["site_period_text"] = text
    start_dt, end_dt = parse_period(text)
    summary = [
        "–ü—Ä–æ–≤–µ—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã üëá",
        f"‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {len(context.user_data.get('site_urls', []))} —Å–∞–π—Ç(–æ–≤)",
        f"‚Ä¢ –ü–µ—Ä–∏–æ–¥: {period_human(start_dt, end_dt)}",
        "–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥?"
    ]
    await update.message.reply_text("\n".join(summary),
                                    reply_markup=InlineKeyboardMarkup([
                                        [InlineKeyboardButton("‚ñ∂Ô∏è –ó–∞–ø—É—Å—Ç–∏—Ç—å", callback_data="site:run")],
                                        [InlineKeyboardButton("‚ùå –û—Ç–º–µ–Ω–∞", callback_data="site:cancel")],
                                    ]))
    return SITE_CONFIRM

def sources_from_rows(rows: List[Dict[str, str]]) -> List[str]:
    chips = []
    for r in rows:
        src = (r.get("source","") or "")
        if src:
            src = re.sub(r"^https?://", "", src).strip("/")
            if src and src not in chips:
                chips.append(src)
    return chips

async def site_confirm(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    if query.data == "site:cancel":
        await query.edit_message_text("–û—Ç–º–µ–Ω–∏–ª. /start ‚Äî –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –º–µ–Ω—é.")
        return ConversationHandler.END
    if query.data != "site:run":
        await query.edit_message_text("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. /start ‚Äî –∑–∞–Ω–æ–≤–æ.")
        return ConversationHandler.END

    user_id = query.from_user.id
    workdir = user_workdir(user_id)
    args_list = build_site_args_from_context(context.user_data)

    await query.edit_message_text("–ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–æ–≤‚Ä¶ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.")

    rc, out, err = await run_site_script(args_list, workdir)
    out_dir = workdir / "output"
    rows = read_all_sites_csv(out_dir)

    start_dt, end_dt = parse_period(context.user_data.get("site_period_text",""))
    period_str = period_human(start_dt, end_dt)
    posts = site_rows_to_posts(rows)
    html = render_html_sites(period_str, sources_chips=sources_from_rows(rows), posts=posts)

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    fname = f"Sites__{start_dt.date()}_{(end_dt - timedelta(days=1)).date()}__{ts}.html"
    fpath = OUTPUT_DIR / fname
    fpath.write_text(html, encoding="utf-8")

    await query.message.reply_document(
        document=fpath.open("rb"),
        filename=fname,
        caption=f"–ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–µ–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(posts)} (rc={rc})\n/start ‚Äî –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –º–µ–Ω—é."
    )
    return ConversationHandler.END

# ---------- LIFECYCLE ----------
async def on_start(app: Application):
    if not tg_client.is_connected():
        await tg_client.connect()
    if not await tg_client.is_user_authorized():
        raise RuntimeError("Telethon –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω. –ü–µ—Ä–µ—Å–æ–∑–¥–∞–π—Ç–µ TELETHON_SESSION —á–µ—Ä–µ–∑ generate_session.py")

async def on_stop(app: Application):
    if tg_client.is_connected():
        await tg_client.disconnect()

def build_application() -> Application:
    try:
        import certifi
        from telegram.request import HTTPXRequest
        req = HTTPXRequest(http2=False, verify=certifi.where(), timeout=30.0, trust_env=True)
        log.info("HTTPXRequest —Å certifi –≤–∫–ª—é—á—ë–Ω.")
        return (
            Application.builder()
            .token(BOT_TOKEN)
            .request(req)
            .post_init(on_start)
            .post_shutdown(on_stop)
            .build()
        )
    except Exception as e:
        log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å HTTPXRequest —Å certifi: {e}. –ò—Å–ø–æ–ª—å–∑—É—é –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
        return (
            Application.builder()
            .token(BOT_TOKEN)
            .post_init(on_start)
            .post_shutdown(on_stop)
            .build()
        )

def main():
    application = build_application()

    conv = ConversationHandler(
        entry_points=[CommandHandler("start", start), CommandHandler("parse", parse_cmd)],
        states={
            MENU: [CallbackQueryHandler(menu_choice, pattern=r"^menu:(tg|site)$")],
            # –¢–ì
            LINK:     [MessageHandler(filters.TEXT & ~filters.COMMAND, ask_period)],
            PERIOD:   [MessageHandler(filters.TEXT & ~filters.COMMAND, ask_keywords)],
            KEYWORDS: [MessageHandler(filters.TEXT & ~filters.COMMAND, run_parse_tg)],
            # –°–∞–π—Ç—ã
            SITE_SITES:  [MessageHandler((filters.Document.ALL | (filters.TEXT & ~filters.COMMAND)), site_collect_sites)],
            SITE_PERIOD: [MessageHandler(filters.TEXT & ~filters.COMMAND, site_collect_period)],
            SITE_CONFIRM:[CallbackQueryHandler(site_confirm, pattern=r"^site:(run|cancel)$")],
        },
        fallbacks=[CommandHandler("cancel", cancel), CommandHandler("start", start)],
        conversation_timeout=900,
    )

    application.add_handler(conv)
    application.run_polling(close_loop=False)

if __name__ == "__main__":
    main()

